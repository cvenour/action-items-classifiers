{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\chris\\anaconda3\\envs\\tf2019\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "import spacy\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import xgboost as xgb\n",
    "from sklearn.externals import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "stemmer = PorterStemmer()\n",
    "vocabularySize = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./action-items-data-chris/train_balanced_4.csv')\n",
    "df_dev = pd.read_csv('./action-items-data-chris/dev_balanced_4.csv')\n",
    "df_test = pd.read_csv('./action-items-data-chris/test_balanced_labelled_4.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1698\n",
      "300\n",
      "184\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train))\n",
    "print(len(df_dev))\n",
    "print(len(df_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1998\n"
     ]
    }
   ],
   "source": [
    "df_train_and_dev = pd.concat([df_train, df_dev]) #let's combine the utterances in the train and dev set\n",
    "#Why? Because we want to create a vocabulary of the most common words that occur in those utterances\n",
    "\n",
    "print(len(df_train_and_dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRidContractions(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    phrase = re.sub(\"could\", \"can\", phrase)\n",
    "    return phrase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countWords(utterances, nlp):\n",
    "    listOfWords = []\n",
    "    \n",
    "    for utterance in utterances:\n",
    "        utt1 = utterance.lower() #we turn the utterance to lowercase\n",
    "        utt2 = getRidContractions(utt1) #we get rid of all contractions\n",
    "        doc = nlp(utt2) #this uses spacy to tokenize a sentence\n",
    "        for token in doc:\n",
    "            if (not token.is_punct): #we ignore punctuation tokens\n",
    "                tokenStemmed = stemmer.stem(token.text) #we stem each word to increase generalization\n",
    "                listOfWords.append(tokenStemmed) #we create a list of every word we come across. There are\n",
    "                #repeats in this list\n",
    "    \n",
    "    wordsAndFrequencies = collections.Counter(listOfWords) #this counts how many times the word 'tree' for example\n",
    "    #occurs in the list called listOfWords\n",
    "    \n",
    "    #wordsAndFrequencies will look something like this: Counter({'is': 487, 'have': 219, 'are': 199, 'be': 148, ...\n",
    "    return wordsAndFrequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessUtterance(utterance, nlp):\n",
    "    #this function turns an utterance into lower case, calls getRidContractions to turn\n",
    "    #contractions into their long forms (e.g. won't -> will not), gets rid of punctuation\n",
    "    #and then uses an nltk function to stem each word.\n",
    "    \n",
    "    listOfStemmedTokens = []\n",
    "    utt1 = utterance.lower()\n",
    "    utt2 = getRidContractions(utt1)\n",
    "    doc = nlp(utt2)\n",
    "    for token in doc:\n",
    "        if (not token.is_punct): #we ignore punctuation tokens\n",
    "            tokenStemmed = stemmer.stem(token.text) #we stem each word to increase generalization\n",
    "            listOfStemmedTokens.append(tokenStemmed)\n",
    "            \n",
    "    return listOfStemmedTokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's find out what the 500 most common words are in the training and dev set (combined)\n",
    "We'll call this list of 500 words the vocabulary.\n",
    "For each utterance, we'll create a 500 dimensional one-hot vector. A 1 in the kth spot in the vector means the utterance contains that kth vocabulary word. A 0 means that vocabulary word doesn't occur in the utterance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAndDevUtterances = df_train_and_dev['utterance'].tolist()\n",
    "#justActionUtterances = df_train_and_dev['utterance'][df_train_and_dev.is_action != 0].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsFreqs = countWords(trainAndDevUtterances, nlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'to', 'i', 'and', 'that', 'is', 'we', 'you', 'a', 'it', 'of', 'so', 'have', 'in', 'will', 'on', 'can', 'are', 'do', 'be', 'for', 'just', 'with', 'not', 'if', 'get', 'what', 'go', 'like', 'uh', 'thi', 'then', 'they', 'know', 'up', 'am', 'there', 'or', 'but', 'um', 'work', 'think', 'yeah', 'at', 'as', 'would', 'some', 'out', 'one', 'need', 'want', 'meet', 'wa', 'look', 'thing', 'about', 'them', 'gon', 'na', 'our', 'from', 'well', 'me', 'those', 'now', 'becaus', 'all', 'send', 'see', 'an', 'right', 'how', 'got', 'your', 'when', 'make', 'time', 'realli', 'call', 'he', 'my', 'week', 'over', 'tri', 'okay', 'more', 'back', 'kind', 'talk', 'here', 'someth', 'let', 'actual', 'next', 'say', 'probabl', 'come', ' ', 'put', 'peopl', 'use', 'start', 'where', 'take', 'should', 'were', 'also', 'mayb', 'into', 'by', 'stuff', 'sure', 'email', 'good', 'other', 'guy', 'these', 'us', 'done', 'through', 'their', 'give', 'first', 'littl', 'build', 'him', 'did', 'had', 'ani', 'today', 'two', 'help', 'down', 'who', 'been', 'way', 'no', 'bit', 'end', 'year', 'ask', 'ha', 'off', 'abl', 'plan', 'compani', 'after', 'point', 'new', 'list', 'hope', 'data', 'coupl', 'which', 'day', 'follow', 'mean', 'find', 'event', 'lot', 'happen', 'busi', 'convers', 'she', 'set', 'chang', 'interest', 'still', 'product', 'befor', 'sort', 'figur', 'whi', 'finish', 'question', 'test', 'veri', 'around', 'even', 'much', 'move', 'team', 'said', 'differ', 'her', 'keep', 'few', 'too', 'number', 'check', 'summari', 'updat', 'togeth', 'three', 'mani', 'doe', 'basic', 'touch', 'term', 'might', 'alreadi', 'instal', 'inform', 'readi', 'connect', 'line', 'document', 'paul', 'pretti', 'group', \"'caus\", 'hour', 'everyth', 'most', 'anoth', 'than', 'market', 'anyth', 'item', 'hi', 'onli', 'room', 'note', 'user', 'part', 'share', 'oh', 'servic', 'definit', 'problem', 'ta', 'provid', '  ', 'review', 'person', 'long', 'may', 'bring', 'monday', 'type', 'run', 'month', 'spend', 'project', 'wanna', 'tomorrow', 'discuss', 'great', 'last', 'action', 'better', 'guess', 'creat', 'issu', 'specif', 'again', 'friday', 'idea', 'schedul', 'report', 'quick', 'post', 'date', 'train', 'everi', 'custom', 'forward', 'phone', 'morn', 'whether', 'develop', 'hendrick', 'far', 'program', 'ibm', 'switch', 'tell', 'ad', 'websit', 'organ', 'floor', 'present', 'add', 'tool', 'edmonton', 'invit', 'whatev', 'cost', 'earli', 'account', 'name', 'model', 'system', 'calendar', 'onc', 'expect', 'import', 'minut', 'big', 'afternoon', 'between', 'copi', 'continu', 'show', 'feel', 'pick', 'els', 'everybodi', 'second', 'place', 'wait', 'same', 'feedback', 'sit', 'walk', 'within', 'respons', 'half', 'manag', 'process', 'link', 'sent', 'write', 'deal', 'leav', 'open', 'whole', 'yet', 'client', 'network', 'away', 'space', 'key', 'engag', 'cool', 'ye', 'commun', 'contract', 'clean', 'anyway', 'thought', 'four', 'somebodi', 'five', 'offic', 'survey', '10', 'detail', 'chat', 'file', 'hard', 'side', 'drive', 'each', 'pull', 'l', 'draft', 'push', 'yesterday', 'student', 'enterpris', 'exampl', 'either', 'access', 'johnni', 'kinda', 'understand', 'board', 'structur', 'step', 'current', 'power', 'technolog', 'base', 'thursday', 'valu', 'money', 'mind', 'complet', 'rememb', 'gener', 'happi', 'speak', 'best', 'david', 'dashboard', 'involv', 'head', 'soon', 'exist', 'track', 'target', 'agenda', 'hendrix', 'suggest', 'went', 'strategi', 'decemb', 'left', 'listen', 'secur', 'until', 'hey', 'fit', 'myself', 'bunch', 'learn', 'least', 'reach', 'potenti', 'usual', 'partner', 'across', 'result', 'direct', 'audio', 'order', 'assign', 'focu', 'watson', 'fund', 'screen', 'area', 'round', 'word', 'replac', 'reason', 'ai', 'obvious', 'summar', 'requir', 'cover', 'locat', 'cat', 'alberta', 'prepar', 'dave', 'quit', 'address', 'topic', 'piec', 'respond', 'sens', 'mostli', 'under', 'pleas', 'deploy', 'demo', 'greg', 'turn', 'form', 'rack', 'improv', 'sound', 'caus', 'june', 'patch', 'top', 'alway', 'contact', 'photo', 'sell', 'later', 'intellig', 'book', 'sometim', 'almost', 'job', 'free', 'renew', 'bar', 'past', 'slide', 'complianc', 'sinc', 'refer', 'text', 'care', 'wednesday', 'sorri', 'festiv', 'suppos', 'includ', 'databas', 'enter', 'industri', 'evalu', 'level', 'budget', 'john', 'ip', 'cabl', 'licens', 'chanc', 'while', 'jump', 'especi', 'languag', 'thank', 'speaker']\n",
      "[1496, 1443, 1384, 1195, 1097, 936, 880, 840, 772, 706, 643, 553, 539, 500, 497, 450, 427, 425, 400, 362, 352, 337, 328, 319, 297, 279, 267, 266, 258, 254, 252, 245, 228, 216, 216, 210, 197, 195, 194, 191, 184, 174, 174, 171, 170, 164, 158, 157, 155, 152, 150, 147, 144, 139, 139, 136, 128, 116, 116, 112, 110, 110, 110, 109, 108, 108, 107, 105, 103, 102, 102, 102, 99, 96, 95, 93, 92, 91, 90, 90, 87, 86, 84, 84, 84, 83, 82, 82, 80, 80, 79, 77, 76, 76, 75, 75, 75, 72, 72, 70, 69, 68, 68, 66, 66, 65, 64, 64, 62, 62, 61, 61, 59, 58, 58, 57, 55, 55, 54, 54, 54, 54, 52, 51, 51, 51, 50, 49, 48, 47, 47, 47, 46, 46, 45, 45, 45, 44, 43, 43, 43, 41, 40, 40, 40, 39, 39, 39, 39, 38, 38, 37, 37, 37, 37, 37, 37, 36, 36, 36, 36, 36, 36, 35, 35, 35, 35, 34, 34, 33, 33, 33, 33, 33, 33, 33, 32, 32, 32, 31, 31, 31, 30, 30, 30, 30, 29, 28, 28, 27, 27, 27, 27, 27, 27, 27, 26, 26, 26, 26, 26, 26, 25, 25, 25, 25, 24, 24, 24, 24, 24, 24, 23, 23, 23, 23, 23, 23, 23, 23, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 20, 20, 20, 20, 20, 20, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 16, 16, 16, 16, 16, 16, 16, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "mostCommonWords = []\n",
    "theirFrequencies = []\n",
    "for wordFreq in wordsFreqs.most_common(vocabularySize):\n",
    "    word = wordFreq[0]\n",
    "    freq = wordFreq[1]\n",
    "    mostCommonWords.append(word)\n",
    "    theirFrequencies.append(freq)\n",
    "    \n",
    "print(mostCommonWords)\n",
    "print(theirFrequencies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OK so we've created a vocabulary of the 500 most frequently occuring words in the action and non-action utterances in the training and dev sets. Now let's create a 500 dimensional one-hot vector for each utterance in the training set. <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createOneHotVector(utterance, nlp, mostCommonWords):\n",
    "    utteranceCleanList = preProcessUtterance(utterance, nlp)\n",
    "    #above turns to lowercase, gets rid of contractions and punctuation and then it stems words\n",
    "    \n",
    "    utteranceVectorNumpy = np.zeros(vocabularySize, dtype=int)\n",
    "    utteranceVectorList = utteranceVectorNumpy.tolist()\n",
    "    \n",
    "    for word in utteranceCleanList:\n",
    "        if word in mostCommonWords:\n",
    "            indexPosition = mostCommonWords.index(word)\n",
    "            utteranceVectorList[indexPosition] = 1\n",
    "        else:\n",
    "            #print(word, \"not in vocabulary\")\n",
    "            pass\n",
    "    \n",
    "    return utteranceVectorList\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I don't use this function anymore. It counted the number of named entities in an utterance and these\n",
    "#frequencies were additional features the algorithms used to decide whether an utterance is an \n",
    "#action item or not. However using this information degraded the classifiers' performance and so I \n",
    "#stopped using named entity frequencies as a feature.\n",
    "\n",
    "def countEntities(utterance,nlp):\n",
    "    time_ents = 0\n",
    "    person_ents = 0\n",
    "    I = 0\n",
    "\n",
    "    doc = nlp(utterance)\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ in [\"DATE\", \"TIME\"]:\n",
    "            #print(\"time entity:\", entity)\n",
    "            time_ents = time_ents+1\n",
    "        elif entity.label_ in [\"PERSON\"]:\n",
    "            #print(\"person entity:\", entity)\n",
    "            person_ents = person_ents+1\n",
    "            \n",
    "    utteranceCleanList = preProcessUtterance(utterance, nlp)\n",
    "    I = utteranceCleanList.count('i')\n",
    "                \n",
    "    return (time_ents,person_ents,I)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1698,)\n",
      "[1 0 0 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "trainUtterances = df_train['utterance'].tolist()\n",
    "trainLabels = df_train['is_action'].tolist()\n",
    "Ynumpy = np.asarray(trainLabels)\n",
    "print(Ynumpy.shape)\n",
    "print(Ynumpy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for utt in trainUtterances:\n",
    "    #entityFrequencies = countEntities(utt,nlp) #returns a tuple\n",
    "    #entityFrequencies = list(entityFrequencies)\n",
    "    vocabularyVector = createOneHotVector(utt, nlp, mostCommonWords)\n",
    "    #finalVector = entityFrequencies + vocabularyVector\n",
    "    X.append(vocabularyVector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1698, 500)\n"
     ]
    }
   ],
   "source": [
    "Xnumpy = np.asarray(X)\n",
    "print(Xnumpy.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.81657848 0.7614841  0.80884956]\n",
      "Mean: 0.7956373799024011\n",
      "Standard Deviation: 0.02435527353687716\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "data_dmatrix_new = xgb.DMatrix(data=Xnumpy, label=Ynumpy)\n",
    "xgb_cv_new = xgb.XGBClassifier(\n",
    " learning_rate=0.1,\n",
    " num_class=2,\n",
    " n_estimators=200,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'multi:softmax',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27\n",
    ")\n",
    "scores = cross_val_score(xgb_cv_new, Xnumpy,Ynumpy, cv=3, scoring = \"accuracy\")\n",
    "print(\"Scores:\", scores)\n",
    "print(\"Mean:\", scores.mean())\n",
    "print(\"Standard Deviation:\", scores.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's find out which words the decision tree finds most useful for distinguishing between action and not action items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "58\n",
      "16\n",
      "91\n",
      "183\n",
      "49\n",
      "93\n",
      "67\n",
      "52\n",
      "173\n",
      "57\n",
      "296\n",
      "97\n",
      "59\n",
      "2\n",
      "215\n",
      "81\n",
      "166\n",
      "212\n",
      "104\n"
     ]
    }
   ],
   "source": [
    "from xgboost import plot_importance\n",
    "from matplotlib import pyplot\n",
    "xgb_cv_new.fit(Xnumpy,Ynumpy)\n",
    "sorted_idx = np.argsort(xgb_cv_new.feature_importances_)[::-1]\n",
    "for index in sorted_idx[0:20]:\n",
    "    print(index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words most useful in distinguishing between action and not action items are:\n",
      "will\n",
      "na\n",
      "can\n",
      "let\n",
      "differ\n",
      "need\n",
      "next\n",
      "send\n",
      "wa\n",
      "finish\n",
      "gon\n",
      "import\n",
      " \n",
      "our\n",
      "i\n",
      "than\n",
      "week\n",
      "interest\n",
      "everyth\n",
      "should\n"
     ]
    }
   ],
   "source": [
    "print(\"The words most useful in distinguishing between action and not action items are:\")\n",
    "most_important_distinguishers = sorted_idx[0:20]\n",
    "for i in most_important_distinguishers:\n",
    "    print(mostCommonWords[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgb_chris_spam_model.joblib.dat']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgb_cv_new, \"xgb_chris_spam_model.joblib.dat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how the xgboost classifier performs on the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(\"xgb_chris_spam_model.joblib.dat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "devUtterances = df_dev['utterance'].tolist()\n",
    "devLabels = df_dev['is_action'].tolist()\n",
    "Ydev = np.asarray(devLabels)\n",
    "print(len(devUtterances))\n",
    "print(len(Ydev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdev = []\n",
    "for utt in devUtterances:\n",
    "    #entityFrequencies = countEntities(utt,nlp) #returns a tuple\n",
    "    #entityFrequencies = list(entityFrequencies)\n",
    "    vocabularyVector = createOneHotVector(utt, nlp, mostCommonWords)\n",
    "    #finalVector = entityFrequencies + vocabularyVector\n",
    "    Xdev.append(vocabularyVector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 500)\n"
     ]
    }
   ],
   "source": [
    "Xdev = np.asarray(Xdev)\n",
    "#print(Xnumpy)\n",
    "print(Xdev.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "[0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0\n",
      " 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1\n",
      " 1 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 0\n",
      " 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1\n",
      " 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 0 0\n",
      " 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 1 1\n",
      " 1 0 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1\n",
      " 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "Ypred = model.predict(Xdev)\n",
    "print(len(Ypred))\n",
    "print(Ypred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "[0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(len(devLabels))\n",
    "print(devLabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateResults(classifierLabels, humanLabels):\n",
    "    \n",
    "    numPredictions = len(classifierLabels)\n",
    "    truePositives = 0\n",
    "    trueNegatives = 0\n",
    "\n",
    "    falsePositives = 0\n",
    "    falseNegatives = 0\n",
    "    numCorrect = 0\n",
    "    numWrong = 0\n",
    "\n",
    "    for i in range(len(humanLabels)): # assuming the lists are of the same length\n",
    "        if (humanLabels[i]==1):\n",
    "            if (classifierLabels[i] == 1):\n",
    "                truePositives = truePositives + 1\n",
    "                numCorrect = numCorrect + 1\n",
    "            elif(classifierLabels[i] == 0):\n",
    "                falseNegatives = falseNegatives + 1\n",
    "                numWrong = numWrong + 1\n",
    "        elif(humanLabels[i]==0):\n",
    "            if (classifierLabels[i] == 0):\n",
    "                trueNegatives = trueNegatives + 1\n",
    "                numCorrect = numCorrect + 1\n",
    "            elif(classifierLabels[i]==1):\n",
    "                falsePositives = falsePositives + 1\n",
    "                numWrong = numWrong + 1\n",
    "\n",
    "\n",
    "    print(\"true positives:\", truePositives)\n",
    "    print(\"false negatives:\", falseNegatives)\n",
    "    print(\"false positives:\", falsePositives)\n",
    "    print()\n",
    "\n",
    "    accuracy = numCorrect/numPredictions\n",
    "    precision = truePositives/(truePositives + falsePositives)\n",
    "    recall = truePositives/(truePositives + falseNegatives)\n",
    "    \n",
    "    return (accuracy,precision,recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positives: 128\n",
      "false negatives: 22\n",
      "false positives: 33\n",
      "\n",
      "accuracy: 0.8166666666666667\n",
      "precision: 0.7950310559006211\n",
      "recall: 0.8533333333333334\n"
     ]
    }
   ],
   "source": [
    "acc,prec,recall = evaluateResults(Ypred, devLabels)\n",
    "print(\"accuracy:\", acc)\n",
    "print(\"precision:\", prec)\n",
    "print(\"recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how the xgboost classifier performs on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n",
      "184\n"
     ]
    }
   ],
   "source": [
    "testUtterances = df_test['utterance'].tolist()\n",
    "testLabels = df_test['label'].tolist()\n",
    "Ytest = np.asarray(testLabels)\n",
    "print(len(testUtterances))\n",
    "print(len(Ytest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = []\n",
    "for utt in testUtterances:\n",
    "    #entityFrequencies = countEntities(utt,nlp) #returns a tuple\n",
    "    #entityFrequencies = list(entityFrequencies)\n",
    "    vocabularyVector = createOneHotVector(utt, nlp, mostCommonWords)\n",
    "    #finalVector = entityFrequencies + vocabularyVector\n",
    "    Xtest.append(vocabularyVector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(184, 500)\n"
     ]
    }
   ],
   "source": [
    "Xtest = np.asarray(Xtest)\n",
    "#print(Xnumpy)\n",
    "print(Xtest.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n",
      "[1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0\n",
      " 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 1 0 0\n",
      " 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1\n",
      " 1 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0\n",
      " 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "Ytest = model.predict(Xtest)\n",
    "print(len(Ytest))\n",
    "print(Ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n",
      "[1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(len(testLabels))\n",
    "print(testLabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positives: 89\n",
      "false negatives: 19\n",
      "false positives: 14\n",
      "\n",
      "accuracy: 0.8206521739130435\n",
      "precision: 0.8640776699029126\n",
      "recall: 0.8240740740740741\n"
     ]
    }
   ],
   "source": [
    "acc,prec,recall = evaluateResults(Ytest, testLabels)\n",
    "print(\"accuracy:\", acc)\n",
    "print(\"precision:\", prec)\n",
    "print(\"recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train and test a catboost model on the same feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6393364\ttotal: 75.6ms\tremaining: 7.48s\n",
      "1:\tlearn: 0.6153570\ttotal: 88.2ms\tremaining: 4.32s\n",
      "2:\tlearn: 0.5871181\ttotal: 98.4ms\tremaining: 3.18s\n",
      "3:\tlearn: 0.5644229\ttotal: 109ms\tremaining: 2.61s\n",
      "4:\tlearn: 0.5400576\ttotal: 119ms\tremaining: 2.26s\n",
      "5:\tlearn: 0.5284592\ttotal: 129ms\tremaining: 2.03s\n",
      "6:\tlearn: 0.5184721\ttotal: 140ms\tremaining: 1.86s\n",
      "7:\tlearn: 0.5075910\ttotal: 150ms\tremaining: 1.72s\n",
      "8:\tlearn: 0.4980112\ttotal: 160ms\tremaining: 1.61s\n",
      "9:\tlearn: 0.4888311\ttotal: 169ms\tremaining: 1.52s\n",
      "10:\tlearn: 0.4782434\ttotal: 179ms\tremaining: 1.45s\n",
      "11:\tlearn: 0.4695108\ttotal: 188ms\tremaining: 1.38s\n",
      "12:\tlearn: 0.4613087\ttotal: 198ms\tremaining: 1.32s\n",
      "13:\tlearn: 0.4562630\ttotal: 207ms\tremaining: 1.27s\n",
      "14:\tlearn: 0.4422027\ttotal: 217ms\tremaining: 1.23s\n",
      "15:\tlearn: 0.4335709\ttotal: 226ms\tremaining: 1.19s\n",
      "16:\tlearn: 0.4241614\ttotal: 236ms\tremaining: 1.15s\n",
      "17:\tlearn: 0.4168694\ttotal: 247ms\tremaining: 1.12s\n",
      "18:\tlearn: 0.4101756\ttotal: 257ms\tremaining: 1.09s\n",
      "19:\tlearn: 0.4040308\ttotal: 267ms\tremaining: 1.07s\n",
      "20:\tlearn: 0.3989936\ttotal: 287ms\tremaining: 1.08s\n",
      "21:\tlearn: 0.3948358\ttotal: 307ms\tremaining: 1.09s\n",
      "22:\tlearn: 0.3926801\ttotal: 328ms\tremaining: 1.1s\n",
      "23:\tlearn: 0.3859141\ttotal: 342ms\tremaining: 1.08s\n",
      "24:\tlearn: 0.3813888\ttotal: 352ms\tremaining: 1.05s\n",
      "25:\tlearn: 0.3725909\ttotal: 363ms\tremaining: 1.03s\n",
      "26:\tlearn: 0.3708719\ttotal: 373ms\tremaining: 1.01s\n",
      "27:\tlearn: 0.3672056\ttotal: 383ms\tremaining: 986ms\n",
      "28:\tlearn: 0.3594025\ttotal: 394ms\tremaining: 964ms\n",
      "29:\tlearn: 0.3551458\ttotal: 403ms\tremaining: 941ms\n",
      "30:\tlearn: 0.3498348\ttotal: 414ms\tremaining: 921ms\n",
      "31:\tlearn: 0.3475773\ttotal: 423ms\tremaining: 899ms\n",
      "32:\tlearn: 0.3416170\ttotal: 433ms\tremaining: 880ms\n",
      "33:\tlearn: 0.3384168\ttotal: 442ms\tremaining: 859ms\n",
      "34:\tlearn: 0.3367585\ttotal: 452ms\tremaining: 839ms\n",
      "35:\tlearn: 0.3322290\ttotal: 461ms\tremaining: 820ms\n",
      "36:\tlearn: 0.3280684\ttotal: 471ms\tremaining: 802ms\n",
      "37:\tlearn: 0.3253767\ttotal: 481ms\tremaining: 784ms\n",
      "38:\tlearn: 0.3222520\ttotal: 502ms\tremaining: 786ms\n",
      "39:\tlearn: 0.3208995\ttotal: 524ms\tremaining: 787ms\n",
      "40:\tlearn: 0.3186805\ttotal: 537ms\tremaining: 773ms\n",
      "41:\tlearn: 0.3137298\ttotal: 548ms\tremaining: 757ms\n",
      "42:\tlearn: 0.3112888\ttotal: 559ms\tremaining: 741ms\n",
      "43:\tlearn: 0.3045780\ttotal: 569ms\tremaining: 725ms\n",
      "44:\tlearn: 0.3019381\ttotal: 579ms\tremaining: 708ms\n",
      "45:\tlearn: 0.3010092\ttotal: 589ms\tremaining: 691ms\n",
      "46:\tlearn: 0.2998127\ttotal: 598ms\tremaining: 675ms\n",
      "47:\tlearn: 0.2983712\ttotal: 608ms\tremaining: 659ms\n",
      "48:\tlearn: 0.2945528\ttotal: 618ms\tremaining: 643ms\n",
      "49:\tlearn: 0.2910945\ttotal: 628ms\tremaining: 628ms\n",
      "50:\tlearn: 0.2896797\ttotal: 638ms\tremaining: 613ms\n",
      "51:\tlearn: 0.2875949\ttotal: 648ms\tremaining: 598ms\n",
      "52:\tlearn: 0.2847586\ttotal: 657ms\tremaining: 583ms\n",
      "53:\tlearn: 0.2835497\ttotal: 667ms\tremaining: 568ms\n",
      "54:\tlearn: 0.2826446\ttotal: 677ms\tremaining: 554ms\n",
      "55:\tlearn: 0.2813159\ttotal: 686ms\tremaining: 539ms\n",
      "56:\tlearn: 0.2801918\ttotal: 696ms\tremaining: 525ms\n",
      "57:\tlearn: 0.2797496\ttotal: 718ms\tremaining: 520ms\n",
      "58:\tlearn: 0.2786143\ttotal: 740ms\tremaining: 515ms\n",
      "59:\tlearn: 0.2759575\ttotal: 754ms\tremaining: 503ms\n",
      "60:\tlearn: 0.2741998\ttotal: 766ms\tremaining: 490ms\n",
      "61:\tlearn: 0.2736769\ttotal: 775ms\tremaining: 475ms\n",
      "62:\tlearn: 0.2734120\ttotal: 785ms\tremaining: 461ms\n",
      "63:\tlearn: 0.2728880\ttotal: 794ms\tremaining: 447ms\n",
      "64:\tlearn: 0.2689637\ttotal: 805ms\tremaining: 433ms\n",
      "65:\tlearn: 0.2665854\ttotal: 815ms\tremaining: 420ms\n",
      "66:\tlearn: 0.2634059\ttotal: 824ms\tremaining: 406ms\n",
      "67:\tlearn: 0.2620100\ttotal: 834ms\tremaining: 392ms\n",
      "68:\tlearn: 0.2594996\ttotal: 845ms\tremaining: 379ms\n",
      "69:\tlearn: 0.2570499\ttotal: 855ms\tremaining: 366ms\n",
      "70:\tlearn: 0.2546317\ttotal: 864ms\tremaining: 353ms\n",
      "71:\tlearn: 0.2527998\ttotal: 874ms\tremaining: 340ms\n",
      "72:\tlearn: 0.2505779\ttotal: 884ms\tremaining: 327ms\n",
      "73:\tlearn: 0.2490482\ttotal: 894ms\tremaining: 314ms\n",
      "74:\tlearn: 0.2477681\ttotal: 904ms\tremaining: 301ms\n",
      "75:\tlearn: 0.2474726\ttotal: 914ms\tremaining: 289ms\n",
      "76:\tlearn: 0.2451437\ttotal: 935ms\tremaining: 279ms\n",
      "77:\tlearn: 0.2408510\ttotal: 957ms\tremaining: 270ms\n",
      "78:\tlearn: 0.2408427\ttotal: 968ms\tremaining: 257ms\n",
      "79:\tlearn: 0.2400585\ttotal: 980ms\tremaining: 245ms\n",
      "80:\tlearn: 0.2387913\ttotal: 990ms\tremaining: 232ms\n",
      "81:\tlearn: 0.2383809\ttotal: 999ms\tremaining: 219ms\n",
      "82:\tlearn: 0.2374960\ttotal: 1.01s\tremaining: 207ms\n",
      "83:\tlearn: 0.2372786\ttotal: 1.02s\tremaining: 194ms\n",
      "84:\tlearn: 0.2353338\ttotal: 1.03s\tremaining: 182ms\n",
      "85:\tlearn: 0.2347122\ttotal: 1.04s\tremaining: 170ms\n",
      "86:\tlearn: 0.2342564\ttotal: 1.05s\tremaining: 158ms\n",
      "87:\tlearn: 0.2325035\ttotal: 1.07s\tremaining: 145ms\n",
      "88:\tlearn: 0.2320045\ttotal: 1.07s\tremaining: 133ms\n",
      "89:\tlearn: 0.2307846\ttotal: 1.09s\tremaining: 121ms\n",
      "90:\tlearn: 0.2307334\ttotal: 1.09s\tremaining: 108ms\n",
      "91:\tlearn: 0.2290838\ttotal: 1.1s\tremaining: 96.1ms\n",
      "92:\tlearn: 0.2289557\ttotal: 1.12s\tremaining: 84.6ms\n",
      "93:\tlearn: 0.2288969\ttotal: 1.14s\tremaining: 73ms\n",
      "94:\tlearn: 0.2269861\ttotal: 1.17s\tremaining: 61.4ms\n",
      "95:\tlearn: 0.2251801\ttotal: 1.18s\tremaining: 49.1ms\n",
      "96:\tlearn: 0.2236460\ttotal: 1.19s\tremaining: 36.7ms\n",
      "97:\tlearn: 0.2229777\ttotal: 1.2s\tremaining: 24.5ms\n",
      "98:\tlearn: 0.2204155\ttotal: 1.21s\tremaining: 12.3ms\n",
      "99:\tlearn: 0.2190796\ttotal: 1.23s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x10a4da828>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "#test_data = catboost_pool = Pool(X_new, y_new)\n",
    "model2 = CatBoostClassifier(iterations=100, \n",
    "                           depth=5, \n",
    "                           learning_rate=0.5, \n",
    "                           loss_function='Logloss', \n",
    "                           logging_level='Verbose')\n",
    "\n",
    "#train the model\n",
    "model2.fit(Xnumpy,Ynumpy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how catboost does on the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "[1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.\n",
      " 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.\n",
      " 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1.\n",
      " 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0.\n",
      " 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1.\n",
      " 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "Ypred = model2.predict(Xdev)\n",
    "print(len(Ypred))\n",
    "print(Ypred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "[0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(len(devLabels))\n",
    "print(devLabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positives: 118\n",
      "false negatives: 32\n",
      "false positives: 34\n",
      "\n",
      "accuracy: 0.78\n",
      "precision: 0.7763157894736842\n",
      "recall: 0.7866666666666666\n"
     ]
    }
   ],
   "source": [
    "acc,prec,recall = evaluateResults(Ypred, devLabels)\n",
    "print(\"accuracy:\", acc)\n",
    "print(\"precision:\", prec)\n",
    "print(\"recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how catboost does on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n",
      "[1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0.\n",
      " 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0.\n",
      " 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "Ytest = model2.predict(Xtest)\n",
    "print(len(Ytest))\n",
    "print(Ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n",
      "[1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(len(testLabels))\n",
    "print(testLabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positives: 83\n",
      "false negatives: 25\n",
      "false positives: 19\n",
      "\n",
      "accuracy: 0.7608695652173914\n",
      "precision: 0.8137254901960784\n",
      "recall: 0.7685185185185185\n"
     ]
    }
   ],
   "source": [
    "acc,prec,recall = evaluateResults(Ytest, testLabels)\n",
    "print(\"accuracy:\", acc)\n",
    "print(\"precision:\", prec)\n",
    "print(\"recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train and test a Support Vector Machine (SVM) on the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(Xnumpy,Ynumpy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "[1 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 1\n",
      " 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0\n",
      " 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1\n",
      " 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 0\n",
      " 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 1 1 1 0 1\n",
      " 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0\n",
      " 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0\n",
      " 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1\n",
      " 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "Ypred = clf.predict(Xdev)\n",
    "print(len(Ypred))\n",
    "print(Ypred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "[0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(len(devLabels))\n",
    "print(devLabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positives: 122\n",
      "false negatives: 28\n",
      "false positives: 34\n",
      "\n",
      "accuracy: 0.7933333333333333\n",
      "precision: 0.782051282051282\n",
      "recall: 0.8133333333333334\n"
     ]
    }
   ],
   "source": [
    "acc,prec,recall = evaluateResults(Ypred, devLabels)\n",
    "print(\"accuracy:\", acc)\n",
    "print(\"precision:\", prec)\n",
    "print(\"recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how SVM does on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n",
      "[1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0\n",
      " 1 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0\n",
      " 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1\n",
      " 0 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 1 1 1 0\n",
      " 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "Ypred = clf.predict(Xtest)\n",
    "print(len(Ypred))\n",
    "print(Ypred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n",
      "[1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(len(testLabels))\n",
    "print(testLabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positives: 90\n",
      "false negatives: 18\n",
      "false positives: 11\n",
      "\n",
      "accuracy: 0.842391304347826\n",
      "precision: 0.8910891089108911\n",
      "recall: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "acc,prec,recall = evaluateResults(Ypred, testLabels)\n",
    "print(\"accuracy:\", acc)\n",
    "print(\"precision:\", prec)\n",
    "print(\"recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The SVM classifier does the best of all the bag of words classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.60312325711098\n"
     ]
    }
   ],
   "source": [
    "precision=87.6\n",
    "recall=91.7\n",
    "\n",
    "#84.9,83.3\n",
    "#86.4,82.4\n",
    "#86.5,83.3\n",
    "\n",
    "f1 = (2 * precision * recall)/(precision + recall)\n",
    "print(f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
