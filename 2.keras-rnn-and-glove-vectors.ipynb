{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\chris\\anaconda3\\envs\\tf2019\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\chris\\anaconda3\\envs\\tf2019\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\chris\\anaconda3\\envs\\tf2019\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\chris\\anaconda3\\envs\\tf2019\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\chris\\anaconda3\\envs\\tf2019\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\chris\\anaconda3\\envs\\tf2019\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\chris\\anaconda3\\envs\\tf2019\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\chris\\anaconda3\\envs\\tf2019\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\chris\\anaconda3\\envs\\tf2019\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\chris\\anaconda3\\envs\\tf2019\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\chris\\anaconda3\\envs\\tf2019\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\chris\\anaconda3\\envs\\tf2019\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./action-items-data-chris/train_balanced_4.csv')\n",
    "df_dev = pd.read_csv('./action-items-data-chris/dev_balanced_4.csv')\n",
    "df_test = pd.read_csv('./action-items-data-chris/test_balanced_labelled_4.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dir = 'C:/Users/chris/Documents/AI/nlp/glove'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1698\n",
      "300\n",
      "184\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train))\n",
    "print(len(df_dev))\n",
    "print(len(df_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_and_dev = pd.concat([df_train, df_dev]) #let's combine the utterances in the train and dev set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAndDevUtterances = df_train_and_dev['utterance'].tolist()\n",
    "train_dev_labels = df_train_and_dev['is_action'].tolist()\n",
    "\n",
    "trainLabels = df_train['is_action'].tolist()\n",
    "devLabels = df_dev['is_action'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testUtterances = df_test['utterance'].tolist()\n",
    "test_labels = df_test['label'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 60\n",
    "training_samples = len(df_train)\n",
    "validation_samples = len(df_dev)\n",
    "max_words = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3781 unique tokens\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(trainAndDevUtterances) #I could have included test utterances here too I think\n",
    "sequences = tokenizer.texts_to_sequences(trainAndDevUtterances)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Found %s unique tokens\" % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsequences = tokenizer.texts_to_sequences(testUtterances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (1998, 60)\n",
      "Shape of label tensor: (1998,)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences,maxlen=maxlen)\n",
    "labels = np.asarray(train_dev_labels)\n",
    "print(\"Shape of data tensor:\", data.shape)\n",
    "print(\"Shape of label tensor:\", labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test data tensor: (184, 60)\n",
      "Shape of test label tensor: (184,)\n"
     ]
    }
   ],
   "source": [
    "testdata = pad_sequences(testsequences,maxlen=maxlen)\n",
    "testlabels = np.asarray(test_labels)\n",
    "print(\"Shape of test data tensor:\", testdata.shape)\n",
    "print(\"Shape of test label tensor:\", testlabels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples:training_samples + validation_samples]\n",
    "y_val = labels[training_samples:training_samples + validation_samples]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 glove word vectors\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:],dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print(\"Found %s glove word vectors\" % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "#This is where the vocabulary we built with our tokenizer, in which words in our action items\n",
    "#dataset are assigned unique index integers, get assigned their corresponding glove vectors.\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\chris\\anaconda3\\envs\\tf2019\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\chris\\anaconda3\\envs\\tf2019\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\chris\\anaconda3\\envs\\tf2019\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 100))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\chris\\anaconda3\\envs\\tf2019\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\chris\\anaconda3\\envs\\tf2019\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\chris\\anaconda3\\envs\\tf2019\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\chris\\anaconda3\\envs\\tf2019\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 1698 samples, validate on 300 samples\n",
      "Epoch 1/40\n",
      "1698/1698 [==============================] - 5s 3ms/step - loss: 0.6423 - acc: 0.6313 - val_loss: 0.5934 - val_acc: 0.6933\n",
      "Epoch 2/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.5717 - acc: 0.7250 - val_loss: 0.6648 - val_acc: 0.6167\n",
      "Epoch 3/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.5390 - acc: 0.7344 - val_loss: 1.0152 - val_acc: 0.5333\n",
      "Epoch 4/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.5275 - acc: 0.7473 - val_loss: 0.7044 - val_acc: 0.6167\n",
      "Epoch 5/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.5042 - acc: 0.7650 - val_loss: 0.7012 - val_acc: 0.6767\n",
      "Epoch 6/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.4717 - acc: 0.7903 - val_loss: 0.8008 - val_acc: 0.6000\n",
      "Epoch 7/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.4593 - acc: 0.7939 - val_loss: 0.5788 - val_acc: 0.7267\n",
      "Epoch 8/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.4322 - acc: 0.8110 - val_loss: 0.8425 - val_acc: 0.6467\n",
      "Epoch 9/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.4314 - acc: 0.8068 - val_loss: 0.6377 - val_acc: 0.6767\n",
      "Epoch 10/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.3999 - acc: 0.8186 - val_loss: 0.4752 - val_acc: 0.7867\n",
      "Epoch 11/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.3733 - acc: 0.8333 - val_loss: 0.4822 - val_acc: 0.7800\n",
      "Epoch 12/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.3564 - acc: 0.8445 - val_loss: 0.5823 - val_acc: 0.7500\n",
      "Epoch 13/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.3237 - acc: 0.8575 - val_loss: 0.6263 - val_acc: 0.7300\n",
      "Epoch 14/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.3038 - acc: 0.8675 - val_loss: 0.5629 - val_acc: 0.7367\n",
      "Epoch 15/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.2715 - acc: 0.8857 - val_loss: 0.5026 - val_acc: 0.7700\n",
      "Epoch 16/40\n",
      "1698/1698 [==============================] - 4s 3ms/step - loss: 0.2495 - acc: 0.8993 - val_loss: 0.6224 - val_acc: 0.7533\n",
      "Epoch 17/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.2080 - acc: 0.9282 - val_loss: 0.5495 - val_acc: 0.7867\n",
      "Epoch 18/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.1815 - acc: 0.9346 - val_loss: 0.5553 - val_acc: 0.7700\n",
      "Epoch 19/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.1715 - acc: 0.9352 - val_loss: 0.6355 - val_acc: 0.7367\n",
      "Epoch 20/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.1384 - acc: 0.9452 - val_loss: 0.5848 - val_acc: 0.7867\n",
      "Epoch 21/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.1258 - acc: 0.9535 - val_loss: 0.5995 - val_acc: 0.7900\n",
      "Epoch 22/40\n",
      "1698/1698 [==============================] - 4s 3ms/step - loss: 0.1078 - acc: 0.9641 - val_loss: 0.9602 - val_acc: 0.6933\n",
      "Epoch 23/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.0957 - acc: 0.9682 - val_loss: 1.2322 - val_acc: 0.6200\n",
      "Epoch 24/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.0881 - acc: 0.9741 - val_loss: 0.7038 - val_acc: 0.7533\n",
      "Epoch 25/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.0747 - acc: 0.9770 - val_loss: 0.9289 - val_acc: 0.7567\n",
      "Epoch 26/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.0768 - acc: 0.9711 - val_loss: 0.7670 - val_acc: 0.7633\n",
      "Epoch 27/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.0622 - acc: 0.9806 - val_loss: 0.8490 - val_acc: 0.7600\n",
      "Epoch 28/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.0606 - acc: 0.9776 - val_loss: 0.8212 - val_acc: 0.7567\n",
      "Epoch 29/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.0641 - acc: 0.9782 - val_loss: 0.8438 - val_acc: 0.7700\n",
      "Epoch 30/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.0480 - acc: 0.9859 - val_loss: 1.5277 - val_acc: 0.6833\n",
      "Epoch 31/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.0417 - acc: 0.9853 - val_loss: 0.9102 - val_acc: 0.7933\n",
      "Epoch 32/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.0571 - acc: 0.9841 - val_loss: 0.8141 - val_acc: 0.7767\n",
      "Epoch 33/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.0367 - acc: 0.9912 - val_loss: 0.8848 - val_acc: 0.7933\n",
      "Epoch 34/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.0447 - acc: 0.9870 - val_loss: 0.7931 - val_acc: 0.7833\n",
      "Epoch 35/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.0245 - acc: 0.9935 - val_loss: 0.9684 - val_acc: 0.7867\n",
      "Epoch 36/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.0283 - acc: 0.9906 - val_loss: 0.9881 - val_acc: 0.7967\n",
      "Epoch 37/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.0443 - acc: 0.9870 - val_loss: 0.9790 - val_acc: 0.7700\n",
      "Epoch 38/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.0148 - acc: 0.9982 - val_loss: 0.9264 - val_acc: 0.7633\n",
      "Epoch 39/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.0319 - acc: 0.9918 - val_loss: 0.9639 - val_acc: 0.7767\n",
      "Epoch 40/40\n",
      "1698/1698 [==============================] - 4s 2ms/step - loss: 0.0177 - acc: 0.9959 - val_loss: 0.9827 - val_acc: 0.7600\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                   epochs=40,\n",
    "                   batch_size=32,\n",
    "                   validation_data=(x_val, y_val))\n",
    "\n",
    "#model.save_weights('pre_trained_glove_model_weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./rnn-glove-action-item-classifier.h5')  # creates a HDF5 file \n",
    "#This saves and allows loading of whole model: architecture + weights + optimizer state\n",
    "#see https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model to find out\n",
    "#how to save just a model's weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('./rnn-glove-action-item-classifier.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9743937927743663, 0.7771739104519719]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.evaluate(testdata, testlabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n"
     ]
    }
   ],
   "source": [
    "print(loaded_model.metrics_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999905]\n",
      " [0.97770834]\n",
      " [0.99972343]\n",
      " [0.07447377]\n",
      " [0.00279701]\n",
      " [0.5196131 ]\n",
      " [0.4372197 ]\n",
      " [0.99999547]\n",
      " [0.01376092]\n",
      " [0.1564514 ]]\n"
     ]
    }
   ],
   "source": [
    "test_predictions = loaded_model.predict(testdata)\n",
    "print(test_predictions[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 0\n",
      " 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0\n",
      " 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0\n",
      " 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0\n",
      " 1 1 1 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "test_binary_predictions = test_predictions >= 0.5\n",
    "test_binary_predictions = test_binary_predictions.astype(int)\n",
    "test_binary_predictions = test_binary_predictions.flatten()\n",
    "print(test_binary_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0\n",
      " 1 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0\n",
      " 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1\n",
      " 1 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0\n",
      " 1 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(testlabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateResults(classifierLabels, humanLabels):\n",
    "    \n",
    "    numPredictions = len(classifierLabels)\n",
    "    truePositives = 0\n",
    "    trueNegatives = 0\n",
    "\n",
    "    falsePositives = 0\n",
    "    falseNegatives = 0\n",
    "    numCorrect = 0\n",
    "    numWrong = 0\n",
    "\n",
    "    for i in range(len(humanLabels)): # assuming the lists are of the same length\n",
    "        if (humanLabels[i]==1):\n",
    "            if (classifierLabels[i] == 1):\n",
    "                truePositives = truePositives + 1\n",
    "                numCorrect = numCorrect + 1\n",
    "            elif(classifierLabels[i] == 0):\n",
    "                falseNegatives = falseNegatives + 1\n",
    "                numWrong = numWrong + 1\n",
    "        elif(humanLabels[i]==0):\n",
    "            if (classifierLabels[i] == 0):\n",
    "                trueNegatives = trueNegatives + 1\n",
    "                numCorrect = numCorrect + 1\n",
    "            elif(classifierLabels[i]==1):\n",
    "                falsePositives = falsePositives + 1\n",
    "                numWrong = numWrong + 1\n",
    "\n",
    "\n",
    "    print(\"true positives:\", truePositives)\n",
    "    print(\"false negatives:\", falseNegatives)\n",
    "    print(\"false positives:\", falsePositives)\n",
    "    print()\n",
    "\n",
    "    accuracy = numCorrect/numPredictions\n",
    "    precision = truePositives/(truePositives + falsePositives)\n",
    "    recall = truePositives/(truePositives + falseNegatives)\n",
    "    \n",
    "    return (accuracy,precision,recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positives: 90\n",
      "false negatives: 18\n",
      "false positives: 23\n",
      "\n",
      "accuracy: 0.7771739130434783\n",
      "precision: 0.7964601769911505\n",
      "recall: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "acc,prec,recall = evaluateResults(test_binary_predictions, testlabels)\n",
    "print(\"accuracy:\", acc)\n",
    "print(\"precision:\", prec)\n",
    "print(\"recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8144796380090498\n"
     ]
    }
   ],
   "source": [
    "f1 = (2 * prec * recall)/(prec + recall)\n",
    "print(f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
